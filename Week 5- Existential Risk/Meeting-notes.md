# Meeting Notes (2nd August, 2021)

## Group 2

### Discussion on readings

* Existential Risk vs Existential Catastrophy
  * Risk talks about extinction of humanity whereas Catastrophy relates to socio-technological setback, also in moral terms

* Existential Catastrophy vs Global Catastrophy
  * Irreversible setback vs mild setback - analogy with a healthy child who suffers irrepairable brain injury vs one who's hospitalized for a few months etc which compromises the system's potential temporarily
  * Question - Does it account for loss of knowledge with the death of certain cultural lineage? Does it have to involve death of people to be classified as such?

* Anthropogenic vs Natural factors and relative threat to humanity as a species
  * Through rough calculations, it's estimated we face a higher risk of extinction or catastrophy from anthropogenic factors than natural factors (Asteroid collision, Climate change, natural disasters etc...)
  * Going by analogy given in the previous point, another way of phrasing this, raises questions and might give ways to think about it as well: "We live in a society where a young (We're still in humanity's youth) individual is more likely to die by suicide than by natural factors." It's something to think about. How would you turn this around? and minimize the risk of suicidal death, if even if it's accidental suicide - like overdosing on psychedelics etc.

### General Thoughts and Discussion

* Apathy/Lack of active cognitive engagement with the concept
  * Even if we acknowledge the risks, typically we don't think about it too much in our day to day lives.
    * Potential reason is a feeling of helplessness and lack of authority - even if we realize something bad is happening/going to happen, we have little to no power to control it. Most of us don't anyway have any significant influence or say in these matters.
    * There's a lot of game theory here. Even if you want peace yourself, and don't want to weaponize, but if the other party has incentive to harm you, you stand to lose a lot. So, what's the most optimal strategy? - <https://ncase.me/trust> to find more
  
* Potential of nuclear War - Does everyone having nuclear bombs lower the risk of a nuclear war?
  * Probably not the right question to ask. A more interesting thought would how can we create incentive structures, such that there is no incentive for warfare., or atleast something of that scale.

* Privacy - is that a fundamental human need? or we've sort of grown to create this need
  * Context - Increased surveillance to protect the society from bad actors, and potential misuses of this technology to invade privacy and constraint freedom of citizens (Eg. Sesame credit - China)

* AI - Judicial System , Self-driving cars etc.
  * Do you go ahead with technology even if you know it creates new problems in trying to solve the old ones? Example - would you trust the law and judgement with a machine who's more likely to be objective and unbiased, but can sometimes make mistakes and cause innocent people to lose their lives/livelihoods? But on a larger scale it's better overall.
  * Need to take into account the algorithms are actually not unbiased. Especially because it's depending on data collected by humans, and society's biases are creeping into these systems. Which can sometimes actually amplify the biases than actually fixing those.
  * For the example of self-driving cars, there's a standard question that gets asked: Who'd you sue or who'd compensate for your loss if your family member was killed by a self-driving car?
    * Issues like this make us alert and aware of the loopholes and limitations in our current judicial system and it's high time we start challenging the structure of these institutions, and the fundamentals of the society on which these are based.
    * A reasonable strategy is to create hybrid systems which combine the best abilities of humans and machines together. We need to think of better human machine partnerships than human vs machine. Reward functions in the society need to be reevaluated.

* Other issues including bio-engineering, global pandemics, terrorism, totalitarian regimes etc. pose additional threats. Will discuss in more detail next week.
